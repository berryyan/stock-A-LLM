#!/usr/bin/env python3
"""
è‚¡ç¥¨åˆ†æç³»ç»Ÿ - Gitç‰ˆæœ¬ç®¡ç†å‡†å¤‡è„šæœ¬
ç”¨äºåˆ†æé¡¹ç›®ç»“æ„ã€æ¸…ç†æ–‡ä»¶ã€ç”Ÿæˆ.gitignoreç­‰
"""

import os
import re
import json
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Tuple
import hashlib

class ProjectGitPreparer:
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root).absolute()
        self.analysis_results = {
            "core_files": [],
            "test_files": [],
            "temp_files": [],
            "data_files": [],
            "config_files": [],
            "documentation": [],
            "to_archive": [],
            "to_delete": [],
            "to_gitignore": [],
            "duplicate_files": [],
            "large_files": []
        }
        
        # æ ¸å¿ƒæ¨¡å—æ¨¡å¼ï¼ˆåŸºäºå®Œæ•´åˆ†ææ›´æ–°ï¼‰
        self.core_patterns = {
            "modules": [
                r"^(config|database|rag|agents|models|api|utils)/.*\.py$",
                r"^requirements\.txt$",
                r"^setup\.py$",
                r"^README\.md$"
            ],
            "important_scripts": [
                # æœ€æ–°ç‰ˆæœ¬çš„æ ¸å¿ƒè„šæœ¬
                r"^smart_processor_v5_1\.py$",  # æœ€æ–°çš„æ™ºèƒ½å¤„ç†å™¨ï¼ˆä¸‰é˜¶æ®µä¸‹è½½ç­–ç•¥ï¼‰
                r"^document_processor\.py$",     # æ–‡æ¡£å¤„ç†æ ¸å¿ƒï¼ˆæ”¯æŒç‰¹æ®ŠPDFä¸‹è½½ï¼‰
                r"^batch_process_manager\.py$",  # æ‰¹é‡å¤„ç†ç®¡ç†
                r"^milvus_dedup_script_v2\.py$", # å»é‡ç»´æŠ¤è„šæœ¬
                r"^rag_query_interface\.py$",    # RAGæŸ¥è¯¢ç•Œé¢
                # é¡¹ç›®ç®¡ç†å·¥å…·
                r"^project_git_prepare\.py$",    # Gitå‡†å¤‡å·¥å…·ï¼ˆä¿ç•™åœ¨æ ¹ç›®å½•ï¼‰
            ]
        }
        
        # æµ‹è¯•æ–‡ä»¶æ¨¡å¼
        self.test_patterns = [
            r"^test_.*\.py$",
            r"^.*_test\.py$",
            r"^quick_test.*\.py$",
            r"^debug_.*\.py$",
            r"^check_.*\.py$",
            r"^verify_.*\.py$",
            r"^fix_.*\.py$",
            r"^patch_.*\.py$",
            r"^diagnose_.*\.py$",
            r"^analyze_.*\.py$"
        ]
        
        # ä¸´æ—¶/å¼€å‘æ–‡ä»¶æ¨¡å¼
        self.temp_patterns = [
            r"^temp_.*",
            r"^tmp_.*",
            r"^\..*_cache",
            r"__pycache__",
            r"\.pyc$",
            r"\.pyo$",
            r"\.pyd$",
            r"\.so$",
            r"\.egg-info",
            r"\.pytest_cache",
            r"\.coverage"
        ]
        
        # æ•°æ®æ–‡ä»¶æ¨¡å¼
        self.data_patterns = [
            r"^data/",
            r"\.db$",
            r"\.sqlite$",
            r"\.csv$",
            r"\.xlsx?$",
            r"\.pdf$",
            r"\.json$",
            r"\.pkl$",
            r"\.npy$",
            r"\.npz$"
        ]
        
        # é…ç½®æ–‡ä»¶æ¨¡å¼
        self.config_patterns = [
            r"^\.env.*",
            r"^config\..*",
            r".*\.ini$",
            r".*\.yaml$",
            r".*\.yml$",
            r".*\.toml$"
        ]
        
        # æ–‡æ¡£æ¨¡å¼
        self.doc_patterns = [
            r".*\.md$",
            r".*\.rst$",
            r"^docs/",
            r"^LICENSE.*",
            r"^CHANGELOG.*"
        ]
        
        # éœ€è¦å½’æ¡£çš„ä¸€æ¬¡æ€§è„šæœ¬ï¼ˆåŸºäºåˆ†æç»“æœï¼‰
        self.one_time_scripts = [
            # SQL Agentä¿®å¤ç³»åˆ—
            r"^fix_sql_agent_indent\.py$",
            r"^fix_sql_agent_indent_error\.py$",
            r"^quick_patch_sql_agent\.py$",
            r"^safe_patch_sql_agent\.py$",
            r"^safe_restore_sql_agent\.py$",
            r"^patch_sql_agent_opt\.py$",
            # APIå“åº”æ ¼å¼ä¿®å¤
            r"^fix_api_response\.py$",
            r"^final_fix_return_format\.py$",
            r"^auto_fix_hybrid\.py$",
            r"^manual_fix_guide\.py$",
            r"^fix_remaining_issue\.py$",
            # æšä¸¾å’Œç±»å‹ä¿®å¤
            r"^fix_hybrid_agent_enum\.py$",
            r"^fix_query_type\.py$",
            r"^temporary_fix\.py$",
            # è¶…æ—¶å’Œé…ç½®ä¿®å¤
            r"^fix_timeout_simple\.py$",
            r"^fix_timeout_issue\.py$",
            r"^patch_api_timeout_opt\.py$",
            r"^patch_config\.py$",
            # å¯¼å…¥ä¿®å¤
            r"^fix_import\.py$",
            r"^fix_imports\.py$",
            # å…¶ä»–ä¿®å¤
            r"^final_fixes\.py$",
            r"^quick_fixes\.py$",
            r"^test_sql_agent_fix\.py$"
        ]
        
        # éœ€è¦å½’æ¡£çš„æ—§ç‰ˆæœ¬æ¨¡å¼ï¼ˆåŸºäºé¡¹ç›®å†å²ï¼‰
        self.archive_patterns = [
            # å¤„ç†å™¨å†å²ç‰ˆæœ¬
            r"^smart_processor\.py$",        # v1ç‰ˆæœ¬
            r"^smart_processor_v2\.py$",     # v2ç‰ˆæœ¬
            r"^smart_processor_v3\.py$",     # v3ç‰ˆæœ¬ï¼ˆè¢«v5_1å–ä»£ï¼‰
            r"^smart_processor_v3_bak\.py$", # v3å¤‡ä»½
            r"^smart_processor_v4\.py$",     # v4ç‰ˆæœ¬
            r"^smart_processor_v5\.py$",     # v5ç‰ˆæœ¬ï¼ˆè¢«v5_1å–ä»£ï¼‰
            # å…¶ä»–æ—§ç‰ˆæœ¬
            r"^milvus_dedup_script\.py$",    # v1ç‰ˆæœ¬ï¼ˆè¢«v2å–ä»£ï¼‰
            r"^dedup_tool\.py$",             # æ—§çš„å»é‡å·¥å…·
            r"^continue_processing\.py$",    # æ—§çš„ç»­å¤„ç†è„šæœ¬
            r"^create_essential_tables\.py$", # ä¸´æ—¶è¡¨åˆ›å»ºè„šæœ¬
            r"^sample_queries\.py$",         # ç¤ºä¾‹æŸ¥è¯¢è„šæœ¬
            r"^check_database_tables\.py$",  # è¢«fixedç‰ˆæœ¬å–ä»£
            # é€šç”¨æ—§ç‰ˆæœ¬æ¨¡å¼
            r".*_old\.py$",
            r".*_backup\.py$",
            r".*\(copy\).*",
            r".*_deprecated.*"
        ]
        
        # å¯é‡å¤ä½¿ç”¨çš„å·¥å…·è„šæœ¬ï¼ˆåŸºäºåˆ†æç»“æœï¼‰
        self.reusable_tools = [
            # æ ¸å¿ƒå·¥å…·
            r"^test_api_correct\.py$",
            r"^tushare-db-analyzer\.py$", 
            r"^analyze_announcement_titles\.py$",
            r"^verify_system\.py$",
            r"^load_milvus_collection\.py$",
            r"^test_agents\.py$",
            r"^final_api_test\.py$",
            # è°ƒè¯•è¯Šæ–­å·¥å…·
            r"^debug_batch_processor\.py$",
            r"^diagnose_agent_issue\.py$",
            r"^check_sql_agent\.py$",
            r"^check_api_enums\.py$",
            r"^check_database_tables_fixed\.py$",
            r"^test_cninfo_pdf\.py$",
            r"^test_pdf_case\.py$",
            # æµ‹è¯•å·¥å…·
            r"^test_components\.py$",
            r"^test_timeout_fixed\.py$",
            r"^minimal_api_test\.py$",
            r"^optimized_test\.py$",
            # å…¶ä»–å·¥å…·
            r"^analyze_scripts\.py$",
            r"^optimize_api_system\.py$",
            r"^verify_api\.py$",
            r"^test_gpu\.py$",
            r"^install_dependencies\.py$"
        ]
        
        # å¤§æ–‡ä»¶é˜ˆå€¼ (10MB)
        self.large_file_threshold = 10 * 1024 * 1024

    def analyze_project(self):
        """åˆ†ææ•´ä¸ªé¡¹ç›®ç»“æ„"""
        print("ğŸ” å¼€å§‹åˆ†æé¡¹ç›®ç»“æ„...")
        
        all_files = []
        for root, dirs, files in os.walk(self.project_root):
            # è·³è¿‡å·²çŸ¥çš„ä¸´æ—¶ç›®å½•
            dirs[:] = [d for d in dirs if not any(
                re.match(pattern, d) for pattern in self.temp_patterns
            )]
            
            for file in files:
                file_path = Path(root) / file
                rel_path = file_path.relative_to(self.project_root)
                all_files.append(rel_path)
        
        # åˆ†ç±»æ–‡ä»¶
        for file_path in all_files:
            self._categorize_file(file_path)
        
        # æ£€æµ‹é‡å¤æ–‡ä»¶
        self._find_duplicates()
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report()

    def _categorize_file(self, file_path: Path):
        """å¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç±»"""
        str_path = str(file_path).replace('\\', '/')
        full_path = self.project_root / file_path
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        try:
            file_size = full_path.stat().st_size
            if file_size > self.large_file_threshold:
                self.analysis_results["large_files"].append({
                    "path": str_path,
                    "size_mb": round(file_size / (1024 * 1024), 2)
                })
        except:
            pass
        
        # ä¼˜å…ˆçº§åˆ†ç±»
        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶æ–‡ä»¶
        if any(re.match(pattern, str_path) for pattern in self.temp_patterns):
            self.analysis_results["to_delete"].append(str_path)
            return
        
        # 2. æ£€æŸ¥æ˜¯å¦æ˜¯æ ¸å¿ƒæ–‡ä»¶
        is_core = False
        for pattern in self.core_patterns["modules"] + self.core_patterns["important_scripts"]:
            if re.match(pattern, str_path):
                self.analysis_results["core_files"].append(str_path)
                is_core = True
                break
        
        if is_core:
            return
        
        # 3. æ£€æŸ¥æ˜¯å¦æ˜¯å¯é‡å¤ä½¿ç”¨çš„å·¥å…·
        if any(re.match(pattern, str_path) for pattern in self.reusable_tools):
            self.analysis_results["test_files"].append(str_path)  # æ ‡è®°ä¸ºå¯ä¿ç•™çš„æµ‹è¯•/å·¥å…·
            return
        
        # 4. æ£€æŸ¥æ˜¯å¦æ˜¯ä¸€æ¬¡æ€§è„šæœ¬
        if any(re.match(pattern, str_path) for pattern in self.one_time_scripts):
            self.analysis_results["to_archive"].append(str_path)
            return
        
        # 5. æ£€æŸ¥æ˜¯å¦éœ€è¦å½’æ¡£ï¼ˆæ—§ç‰ˆæœ¬ï¼‰
        if any(re.match(pattern, str_path) for pattern in self.archive_patterns):
            self.analysis_results["to_archive"].append(str_path)
            return
        
        # 6. æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ–‡ä»¶
        if any(re.match(pattern, str_path) for pattern in self.test_patterns):
            self.analysis_results["test_files"].append(str_path)
            return
        
        # 7. æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®æ–‡ä»¶
        if any(re.match(pattern, str_path) for pattern in self.data_patterns):
            self.analysis_results["data_files"].append(str_path)
            self.analysis_results["to_gitignore"].append(str_path)
            return
        
        # 8. æ£€æŸ¥æ˜¯å¦æ˜¯é…ç½®æ–‡ä»¶
        if any(re.match(pattern, str_path) for pattern in self.config_patterns):
            self.analysis_results["config_files"].append(str_path)
            if ".env" in str_path and not str_path.endswith('.example'):
                self.analysis_results["to_gitignore"].append(str_path)
            return
        
        # 9. æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡æ¡£
        if any(re.match(pattern, str_path) for pattern in self.doc_patterns):
            self.analysis_results["documentation"].append(str_path)
            # é¡¹ç›®çŠ¶æ€æ–‡æ¡£åº”è¯¥ä¿ç•™
            if "project_status" in str_path:
                self.analysis_results["core_files"].append(str_path)
            return
        
        # 10. å…¶ä»–Pythonæ–‡ä»¶å¯èƒ½éœ€è¦æ£€æŸ¥
        if str_path.endswith('.py'):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç‹¬ç«‹çš„æµ‹è¯•/è°ƒè¯•è„šæœ¬
            content = self._check_file_content(full_path)
            if content and self._is_standalone_test_script(content):
                self.analysis_results["test_files"].append(str_path)
            else:
                # å¯èƒ½æ˜¯é—æ¼çš„é‡è¦æ–‡ä»¶ï¼Œéœ€è¦äººå·¥ç¡®è®¤
                print(f"âš ï¸  éœ€è¦äººå·¥ç¡®è®¤: {str_path}")

    def _check_file_content(self, file_path: Path) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            return ""

    def _is_standalone_test_script(self, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯ç‹¬ç«‹çš„æµ‹è¯•è„šæœ¬"""
        test_indicators = [
            r"if __name__ == ['\"]__main__['\"]:",
            r"def test_",
            r"print\(.*(æµ‹è¯•|test|debug)",
            r"# test",
            r"# debug",
            r"# æµ‹è¯•"
        ]
        
        for indicator in test_indicators:
            if re.search(indicator, content, re.IGNORECASE):
                return True
        return False

    def _find_duplicates(self):
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
        file_hashes = {}
        
        for category in ["core_files", "test_files", "to_archive"]:
            for file_path in self.analysis_results[category]:
                full_path = self.project_root / file_path
                if full_path.exists() and full_path.is_file():
                    file_hash = self._get_file_hash(full_path)
                    if file_hash:
                        if file_hash in file_hashes:
                            self.analysis_results["duplicate_files"].append({
                                "file1": file_hashes[file_hash],
                                "file2": file_path,
                                "hash": file_hash
                            })
                        else:
                            file_hashes[file_hash] = file_path

    def _get_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œ"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return ""

    def _generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"git_prepare_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# è‚¡ç¥¨åˆ†æç³»ç»Ÿ - Gitç‰ˆæœ¬ç®¡ç†å‡†å¤‡æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ç»Ÿè®¡æ‘˜è¦
            f.write("## ğŸ“Š æ–‡ä»¶ç»Ÿè®¡æ‘˜è¦\n\n")
            f.write(f"- æ ¸å¿ƒæ–‡ä»¶: {len(self.analysis_results['core_files'])} ä¸ª\n")
            f.write(f"- æµ‹è¯•æ–‡ä»¶: {len(self.analysis_results['test_files'])} ä¸ª\n")
            f.write(f"- å¾…å½’æ¡£æ–‡ä»¶: {len(self.analysis_results['to_archive'])} ä¸ª\n")
            f.write(f"- å¾…åˆ é™¤æ–‡ä»¶: {len(self.analysis_results['to_delete'])} ä¸ª\n")
            f.write(f"- æ•°æ®æ–‡ä»¶: {len(self.analysis_results['data_files'])} ä¸ª\n")
            f.write(f"- é…ç½®æ–‡ä»¶: {len(self.analysis_results['config_files'])} ä¸ª\n")
            f.write(f"- æ–‡æ¡£æ–‡ä»¶: {len(self.analysis_results['documentation'])} ä¸ª\n")
            f.write(f"- é‡å¤æ–‡ä»¶: {len(self.analysis_results['duplicate_files'])} ç»„\n")
            f.write(f"- å¤§æ–‡ä»¶(>10MB): {len(self.analysis_results['large_files'])} ä¸ª\n\n")
            
            # æ ¸å¿ƒæ–‡ä»¶åˆ—è¡¨ï¼ˆåŸºäºé¡¹ç›®çŠ¶æ€æ–‡æ¡£ï¼‰
            f.write("## ğŸ”§ æ ¸å¿ƒæ–‡ä»¶ï¼ˆå¿…é¡»ä¿ç•™ï¼‰\n\n")
            f.write("### æ ¸å¿ƒæ¨¡å—\n")
            core_modules = [f for f in self.analysis_results['core_files'] if '/' in f and not f.endswith('.md')]
            for file in sorted(core_modules):
                f.write(f"- âœ… {file}\n")
            
            f.write("\n### é‡è¦è„šæœ¬ï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰\n")
            core_scripts = [f for f in self.analysis_results['core_files'] if '/' not in f and f.endswith('.py')]
            for file in sorted(core_scripts):
                desc = ""
                if "v5_1" in file:
                    desc = " (ä¸‰é˜¶æ®µPDFä¸‹è½½ç­–ç•¥)"
                elif "document_processor" in file:
                    desc = " (æ”¯æŒå·¨æ½®ç‰¹æ®ŠPDF)"
                elif "milvus_dedup_script_v2" in file:
                    desc = " (å‘é‡å»é‡ç»´æŠ¤)"
                elif "batch_process_manager" in file:
                    desc = " (æ‰¹é‡å¤„ç†ç®¡ç†)"
                elif "rag_query_interface" in file:
                    desc = " (RAGæŸ¥è¯¢ç•Œé¢)"
                elif "test_api_correct" in file:
                    desc = " (APIæµ‹è¯•)"
                f.write(f"- âœ… {file}{desc}\n")
            
            f.write("\n### é¡¹ç›®æ–‡æ¡£\n")
            docs = [f for f in self.analysis_results['core_files'] if f.endswith('.md')]
            for file in sorted(docs):
                f.write(f"- âœ… {file}\n")
            
            # å¯é‡å¤ä½¿ç”¨çš„å·¥å…·ï¼ˆåŸºäºåˆ†æç»“æœåˆ†ç±»ï¼‰
            f.write("\n## ğŸ› ï¸ å¯é‡å¤ä½¿ç”¨çš„å·¥å…·ï¼ˆå»ºè®®ä¿ç•™ï¼‰\n\n")
            f.write("å»ºè®®ç§»åŠ¨åˆ° `scripts/` ç›¸åº”ç›®å½•:\n\n")
            
            # åˆ†ç±»æ˜¾ç¤º
            tool_categories = {
                "åˆ†æå·¥å…·": ["tushare-db-analyzer", "analyze_announcement", "analyze_scripts"],
                "æµ‹è¯•å·¥å…·": ["test_api_correct", "test_agents", "test_components", "final_api_test"],
                "è°ƒè¯•å·¥å…·": ["debug_", "diagnose_", "check_"],
                "ç³»ç»Ÿå·¥å…·": ["verify_system", "load_milvus", "install_dependencies"],
                "å…¶ä»–å·¥å…·": []
            }
            
            reusable = [f for f in self.analysis_results['test_files'] 
                       if any(re.match(p, f) for p in self.reusable_tools)]
            
            categorized = set()
            for category, keywords in tool_categories.items():
                f.write(f"\n### {category}\n")
                for file in sorted(reusable):
                    if any(keyword in file.lower() for keyword in keywords):
                        f.write(f"- âœ… {file} â†’ `scripts/{category.replace('å·¥å…·', '').lower()}/`\n")
                        categorized.add(file)
            
            # æœªåˆ†ç±»çš„
            uncategorized = set(reusable) - categorized
            if uncategorized:
                f.write("\n### å…¶ä»–å·¥å…·\n")
                for file in sorted(uncategorized):
                    f.write(f"- âœ… {file} â†’ `scripts/tools/`\n")
            
            # å¾…å½’æ¡£æ–‡ä»¶ï¼ˆåŸºäºåˆ†æç»“æœï¼‰
            f.write("\n## ğŸ“¦ å¾…å½’æ¡£æ–‡ä»¶ï¼ˆä¸€æ¬¡æ€§è„šæœ¬å’Œæ—§ç‰ˆæœ¬ï¼‰\n\n")
            f.write("å»ºè®®ç§»åŠ¨åˆ° `archive/` ç›¸åº”ç›®å½•:\n")
            
            # åˆ†ç±»å½’æ¡£æ–‡ä»¶
            archive_categories = {
                "SQL Agentä¿®å¤": ["fix_sql_agent", "patch_sql_agent", "safe_patch_sql", "safe_restore_sql"],
                "APIå“åº”ä¿®å¤": ["fix_api_response", "final_fix_return", "auto_fix_hybrid", "manual_fix_guide"],
                "è¶…æ—¶ä¿®å¤": ["fix_timeout", "patch_api_timeout", "patch_config"],
                "å¯¼å…¥ä¿®å¤": ["fix_import"],
                "æšä¸¾ä¿®å¤": ["fix_hybrid_agent_enum", "fix_query_type"],
                "å¤„ç†å™¨æ—§ç‰ˆæœ¬": ["smart_processor", "smart_processor_v"],
                "å…¶ä»–æ—§ç‰ˆæœ¬": []
            }
            
            categorized_archives = set()
            for category, keywords in archive_categories.items():
                category_files = []
                for file in sorted(self.analysis_results['to_archive']):
                    if any(keyword in file.lower() for keyword in keywords):
                        category_files.append(file)
                        categorized_archives.add(file)
                
                if category_files:
                    f.write(f"\n### {category}\n")
                    for file in category_files:
                        f.write(f"- ğŸ“ {file} â†’ `archive/fixes/{category.replace(' ', '_').lower()}/`\n")
            
            # æœªåˆ†ç±»çš„å½’æ¡£æ–‡ä»¶
            uncategorized_archives = set(self.analysis_results['to_archive']) - categorized_archives
            if uncategorized_archives:
                f.write("\n### å…¶ä»–å½’æ¡£æ–‡ä»¶\n")
                for file in sorted(uncategorized_archives):
                    f.write(f"- ğŸ“ {file} â†’ `archive/other/`\n")
            
            # å¾…åˆ é™¤æ–‡ä»¶
            f.write("\n## ğŸ—‘ï¸ å¾…åˆ é™¤æ–‡ä»¶ï¼ˆä¸´æ—¶æ–‡ä»¶ï¼‰\n\n")
            for file in sorted(self.analysis_results['to_delete']):
                f.write(f"- âŒ {file}\n")
            
            # å¤§æ–‡ä»¶è­¦å‘Š
            if self.analysis_results['large_files']:
                f.write("\n## âš ï¸ å¤§æ–‡ä»¶è­¦å‘Š\n\n")
                for file_info in sorted(self.analysis_results['large_files'], 
                                       key=lambda x: x['size_mb'], reverse=True):
                    f.write(f"- {file_info['path']} ({file_info['size_mb']} MB)\n")
            
            # é‡å¤æ–‡ä»¶
            if self.analysis_results['duplicate_files']:
                f.write("\n## ğŸ”„ é‡å¤æ–‡ä»¶\n\n")
                for dup in self.analysis_results['duplicate_files']:
                    f.write(f"- {dup['file1']} = {dup['file2']}\n")
            
            # .gitignore å»ºè®®
            f.write("\n## ğŸ“ .gitignore å»ºè®®\n\n")
            f.write("```gitignore\n")
            gitignore_patterns = self._generate_gitignore_content()
            f.write(gitignore_patterns)
            f.write("```\n")
            
            # é¡¹ç›®ç»“æ„å»ºè®®ï¼ˆåŸºäºæœ€æ–°é¡¹ç›®çŠ¶æ€ï¼‰
            f.write("\n## ğŸ—ï¸ å»ºè®®çš„é¡¹ç›®ç»“æ„\n\n")
            f.write("```\n")
            f.write(self._generate_suggested_structure())
            f.write("```\n")
            
            # æ·»åŠ æ•°æ®ç»Ÿè®¡
            f.write("\n## ğŸ“Š é¡¹ç›®æ•°æ®ç»Ÿè®¡\n\n")
            f.write("åŸºäºé¡¹ç›®çŠ¶æ€æ–‡æ¡£ï¼ˆ2025-06-14ï¼‰:\n")
            f.write("- MySQLè®°å½•æ•°: 2800ä¸‡+æ¡\n")
            f.write("- Milvuså‘é‡æ•°: 95,662+ä¸ª\n")
            f.write("- å·²å¤„ç†å…¬å¸: 500+å®¶\n")
            f.write("- PDFæ–‡æ¡£: æ•°åƒä»½\n")
            f.write("- ä»£ç è§„æ¨¡: çº¦20,000è¡Œ\n")
            
            # æ‰§è¡Œæ­¥éª¤
            f.write("\n## ğŸš€ å»ºè®®çš„æ‰§è¡Œæ­¥éª¤\n\n")
            f.write("1. **å¤‡ä»½å½“å‰é¡¹ç›®**\n")
            f.write("   ```bash\n")
            f.write("   cp -r . ../stock_analysis_backup_$(date +%Y%m%d)\n")
            f.write("   ```\n\n")
            
            f.write("2. **åˆ›å»ºå¿…è¦çš„ç›®å½•**\n")
            f.write("   ```bash\n")
            f.write("   mkdir -p tests archive docs/examples\n")
            f.write("   ```\n\n")
            
            f.write("3. **ç§»åŠ¨æµ‹è¯•æ–‡ä»¶**\n")
            f.write("   ```bash\n")
            f.write("   mv test_*.py tests/\n")
            f.write("   mv *_test.py tests/\n")
            f.write("   ```\n\n")
            
            f.write("4. **å½’æ¡£æ—§ç‰ˆæœ¬**\n")
            f.write("   ```bash\n")
            f.write("   mv *_v[0-4].py archive/  # ä¿ç•™v5_1\n")
            f.write("   mv *_old.py archive/\n")
            f.write("   mv *_backup.py archive/\n")
            f.write("   ```\n\n")
            
            f.write("5. **æ¸…ç†ä¸´æ—¶æ–‡ä»¶**\n")
            f.write("   ```bash\n")
            f.write("   find . -type d -name __pycache__ -exec rm -rf {} +\n")
            f.write("   find . -name '*.pyc' -delete\n")
            f.write("   find . -name '*.pyo' -delete\n")
            f.write("   rm -rf .pytest_cache .coverage\n")
            f.write("   ```\n\n")
            
            f.write("6. **åˆ›å»º.gitignore**\n")
            f.write("   ```bash\n")
            f.write("   python project_git_prepare.py --create-gitignore\n")
            f.write("   ```\n\n")
            
            f.write("7. **åˆå§‹åŒ–Gitä»“åº“**\n")
            f.write("   ```bash\n")
            f.write("   git init\n")
            f.write("   git add .\n")
            f.write("   git commit -m 'Initial commit: Stock Analysis System v1.2'\n")
            f.write("   ```\n\n")
            
            f.write("8. **åˆ›å»ºGitHubä»“åº“å¹¶æ¨é€**\n")
            f.write("   ```bash\n")
            f.write("   git remote add origin https://github.com/YOUR_USERNAME/stock-analysis-system.git\n")
            f.write("   git branch -M main\n")
            f.write("   git push -u origin main\n")
            f.write("   ```\n")
        
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file

    def _generate_gitignore_content(self) -> str:
        """ç”Ÿæˆ.gitignoreå†…å®¹"""
        gitignore_content = """# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual Environment
venv/
ENV/
env/
stock_analysis_env/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.project
.pydevproject

# Testing
.pytest_cache/
.coverage
.tox/
htmlcov/
.hypothesis/

# Logs
logs/
*.log

# Environment variables
.env
.env.*
!.env.example

# Data files
data/
*.db
*.sqlite
*.csv
*.xlsx
*.xls
*.pdf
*.pkl
*.npy
*.npz

# Jupyter Notebook
.ipynb_checkpoints

# macOS
.DS_Store

# Windows
Thumbs.db
ehthumbs.db

# Project specific
archive/
temp/
tmp/
downloads/
output/
backup/

# Large model files
*.bin
*.pth
*.h5
*.safetensors

# Secrets
secrets/
credentials/
*.key
*.pem
"""
        return gitignore_content

    def _generate_suggested_structure(self) -> str:
        """ç”Ÿæˆå»ºè®®çš„é¡¹ç›®ç»“æ„"""
        return """stock_analysis_system/
â”œâ”€â”€ config/                 # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ database/              # æ•°æ®åº“è¿æ¥
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mysql_connector.py
â”‚   â””â”€â”€ milvus_connector.py
â”œâ”€â”€ rag/                   # RAGç³»ç»Ÿ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_processor.py    # ä¸‰é˜¶æ®µPDFä¸‹è½½
â”‚   â””â”€â”€ vector_store.py
â”œâ”€â”€ agents/                # æ™ºèƒ½ä»£ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sql_agent.py             # SQLæŸ¥è¯¢
â”‚   â”œâ”€â”€ rag_agent.py             # RAGæŸ¥è¯¢
â”‚   â””â”€â”€ hybrid_agent.py          # æ··åˆæŸ¥è¯¢
â”œâ”€â”€ models/                # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_models.py            # DeepSeeké…ç½®
â”‚   â”œâ”€â”€ embedding_model.py       # BGE-M3æ¨¡å‹
â”‚   â””â”€â”€ bge-m3/                  # æ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ api/                   # APIæœåŠ¡
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py                  # FastAPIä¸»ç¨‹åº
â”œâ”€â”€ utils/                 # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ helpers.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ performance_tracker.py
â”‚   â””â”€â”€ auto_performance_logger.py
â”œâ”€â”€ scripts/               # ç‹¬ç«‹è„šæœ¬
â”‚   â”œâ”€â”€ smart_processor_v5_1.py  # æœ€æ–°å¤„ç†å™¨
â”‚   â”œâ”€â”€ batch_process_manager.py # æ‰¹é‡ç®¡ç†
â”‚   â”œâ”€â”€ milvus_dedup_script_v2.py # å»é‡å·¥å…·
â”‚   â””â”€â”€ rag_query_interface.py  # æŸ¥è¯¢ç•Œé¢
â”œâ”€â”€ tests/                 # æµ‹è¯•æ–‡ä»¶
â”‚   â”œâ”€â”€ test_api_correct.py      # APIæµ‹è¯•
â”‚   â”œâ”€â”€ test_databases.py        # æ•°æ®åº“æµ‹è¯•
â”‚   â”œâ”€â”€ test_embedding_model.py  # æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ test_cninfo_pdf.py       # PDFæµ‹è¯•
â”‚   â””â”€â”€ test_milvus_connection.py
â”œâ”€â”€ archive/               # å½’æ¡£æ—§ç‰ˆæœ¬
â”‚   â”œâ”€â”€ smart_processor_v1-v3/   # å†å²ç‰ˆæœ¬
â”‚   â”œâ”€â”€ old_tests/               # æ—§æµ‹è¯•æ–‡ä»¶
â”‚   â””â”€â”€ deprecated/              # åºŸå¼ƒä»£ç 
â”œâ”€â”€ docs/                  # æ–‡æ¡£
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ API.md                   # APIæ–‡æ¡£
â”‚   â”œâ”€â”€ deployment.md            # éƒ¨ç½²æŒ‡å—
â”‚   â”œâ”€â”€ examples/                # ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ project_status/          # é¡¹ç›®çŠ¶æ€å†å²
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•ï¼ˆä¸æäº¤ï¼‰
â”‚   â”œâ”€â”€ pdfs/                    # PDFç¼“å­˜
â”‚   â”œâ”€â”€ logs/                    # è¿è¡Œæ—¥å¿—
â”‚   â””â”€â”€ milvus/                  # å‘é‡æ•°æ®
â”œâ”€â”€ requirements.txt       # ä¾èµ–æ¸…å•
â”œâ”€â”€ setup.py              # å®‰è£…é…ç½®
â”œâ”€â”€ .gitignore            # Gitå¿½ç•¥
â”œâ”€â”€ .env.example          # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ LICENSE               # MITè®¸å¯è¯
â””â”€â”€ README.md             # é¡¹ç›®è¯´æ˜"""

    def create_gitignore(self):
        """åˆ›å»º.gitignoreæ–‡ä»¶"""
        gitignore_path = self.project_root / ".gitignore"
        with open(gitignore_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_gitignore_content())
        print(f"âœ… .gitignore æ–‡ä»¶å·²åˆ›å»º: {gitignore_path}")

    def create_env_example(self):
        """åˆ›å»º.env.exampleæ–‡ä»¶"""
        env_example_content = """# MySQLæ•°æ®åº“é…ç½®
MYSQL_HOST=10.0.0.77
MYSQL_PORT=3306
MYSQL_DATABASE=Tushare
MYSQL_USER=your_username
MYSQL_PASSWORD=your_password

# Milvuså‘é‡æ•°æ®åº“é…ç½®
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION_NAME=stock_announcements

# LLMé…ç½® - DeepSeek
DEEPSEEK_API_KEY=your_api_key_here
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# æˆ–è€…ä½¿ç”¨é€šä¹‰åƒé—®
DASHSCOPE_API_KEY=your_api_key_here

# æ–‡æ¡£å­˜å‚¨è·¯å¾„
PDF_STORAGE_PATH=./data/pdfs
VECTOR_DB_PATH=./data/milvus

# ç³»ç»Ÿé…ç½®
LOG_LEVEL=INFO
MAX_WORKERS=4
USE_GPU=false
"""
        env_example_path = self.project_root / ".env.example"
        with open(env_example_path, 'w', encoding='utf-8') as f:
            f.write(env_example_content)
        print(f"âœ… .env.example æ–‡ä»¶å·²åˆ›å»º: {env_example_path}")

    def create_readme(self):
        """åˆ›å»ºREADME.mdæ–‡ä»¶"""
        readme_content = """# è‚¡ç¥¨åˆ†æç³»ç»Ÿ (Stock Analysis System)

åŸºäºLangChainçš„æ™ºèƒ½è‚¡ç¥¨åˆ†æç³»ç»Ÿï¼Œé›†æˆSQLæŸ¥è¯¢ã€RAGæ£€ç´¢å’Œæ··åˆæŸ¥è¯¢åŠŸèƒ½ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

### æ ¸å¿ƒåŠŸèƒ½ï¼ˆ100%å®Œæˆï¼‰
- **SQLæŸ¥è¯¢ç³»ç»Ÿ**: å®æ—¶è‚¡ä»·ã€å†å²æ•°æ®ã€æ’è¡Œç»Ÿè®¡ã€æ•°æ®å¯¹æ¯”
- **RAGæŸ¥è¯¢ç³»ç»Ÿ**: è´¢åŠ¡æŠ¥å‘Šåˆ†æã€å…¬å‘Šå†…å®¹æ£€ç´¢ï¼ˆ95,662+æ–‡æ¡£å·²ç´¢å¼•ï¼‰
- **æ··åˆæŸ¥è¯¢ç³»ç»Ÿ**: æ™ºèƒ½è·¯ç”±ï¼ŒSQL+RAGè‡ªåŠ¨æ•´åˆ
- **APIæœåŠ¡**: RESTfulæ¥å£ï¼Œå®Œæ•´çš„Swaggeræ–‡æ¡£
- **æ–‡æ¡£å¤„ç†**: ä¸‰é˜¶æ®µæ™ºèƒ½PDFä¸‹è½½ç­–ç•¥ï¼Œ100%æˆåŠŸç‡

### æŠ€æœ¯äº®ç‚¹
- æ”¯æŒå·¨æ½®ç½‘ç«™ç‰¹æ®ŠPDFä¸‹è½½ï¼ˆè§£å†³sessionå’Œå¤§å°å†™é—®é¢˜ï¼‰
- BGE-M3å‘é‡æ¨¡å‹ï¼ˆ1024ç»´ï¼‰è¿›è¡Œè¯­ä¹‰æ£€ç´¢
- æ™ºèƒ½å»é‡å’Œæ‰¹é‡å¤„ç†ç®¡ç†
- å®Œæ•´çš„æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

## ğŸ“Š æ•°æ®è§„æ¨¡

- **è‚¡ç¥¨æ•°æ®**: 1500ä¸‡+æ¡æ—¥çº¿è¡Œæƒ…è®°å½•
- **å…¬å‘Šæ–‡æ¡£**: 200ä¸‡+æ¡å…¬å‘Šå…ƒæ•°æ®ï¼Œ95,662+å‘é‡åŒ–æ–‡æ¡£
- **è´¢åŠ¡æ•°æ®**: å®Œæ•´çš„ä¸‰å¤§æŠ¥è¡¨å’Œè´¢åŠ¡æŒ‡æ ‡
- **å®æ—¶æ›´æ–°**: æ”¯æŒæ¯æ—¥æ•°æ®åŒæ­¥

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- MySQL 5.7+
- Milvus 2.0+
- CUDA (å¯é€‰ï¼Œç”¨äºGPUåŠ é€Ÿ)
- 16GB+ RAMæ¨è

## ğŸ”§ å¿«é€Ÿå¼€å§‹

### 1. å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/YOUR_USERNAME/stock-analysis-system.git
cd stock-analysis-system
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
```bash
python -m venv stock_analysis_env
source stock_analysis_env/bin/activate  # Linux/Mac
# æˆ–
stock_analysis_env\\Scripts\\activate  # Windows
```

### 3. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### 4. é…ç½®ç¯å¢ƒå˜é‡
```bash
cp .env.example .env
# ç¼–è¾‘.envæ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„æ•°æ®åº“å’ŒAPIé…ç½®
```

### 5. å¯åŠ¨æœåŠ¡
```bash
python -m uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

è®¿é—® http://localhost:8000/docs æŸ¥çœ‹APIæ–‡æ¡£

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### SQLæŸ¥è¯¢
```python
# æŸ¥è¯¢è´µå·èŒ…å°æœ€æ–°è‚¡ä»·
POST /api/query
{
    "question": "è´µå·èŒ…å°æœ€æ–°è‚¡ä»·",
    "query_type": "sql"
}

# æŸ¥è¯¢æ¶¨å¹…æ’è¡Œ
{
    "question": "ä»Šå¤©æ¶¨å¹…æœ€å¤§çš„10åªè‚¡ç¥¨",
    "query_type": "sql"
}
```

### RAGæŸ¥è¯¢
```python
# æŸ¥è¯¢è´¢åŠ¡æŠ¥å‘Š
POST /api/query
{
    "question": "è´µå·èŒ…å°2024å¹´ç¬¬ä¸€å­£åº¦è¥æ”¶æƒ…å†µ",
    "query_type": "rag"
}

# åˆ†ææ¯›åˆ©ç‡
{
    "question": "åˆ†æç™½é…’è¡Œä¸šä¸»è¦å…¬å¸çš„æ¯›åˆ©ç‡å¯¹æ¯”",
    "query_type": "rag"
}
```

### æ··åˆæŸ¥è¯¢
```python
# ç»¼åˆåˆ†æ
POST /api/query
{
    "question": "åˆ†æè´µå·èŒ…å°çš„è´¢åŠ¡çŠ¶å†µå’Œè‚¡ä»·è¡¨ç°",
    "query_type": "hybrid"
}
```

## ğŸ› ï¸ ä¸»è¦ç»„ä»¶

### æ ¸å¿ƒæ¨¡å—
- `config/` - ç³»ç»Ÿé…ç½®ç®¡ç†
- `database/` - MySQLå’ŒMilvusè¿æ¥å™¨
- `rag/` - RAGç³»ç»Ÿå®ç°
- `agents/` - SQLã€RAGå’Œæ··åˆAgent
- `models/` - LLMå’ŒåµŒå…¥æ¨¡å‹é…ç½®
- `api/` - FastAPIæœåŠ¡æ¥å£
- `utils/` - å·¥å…·å‡½æ•°å’Œæ—¥å¿—ç³»ç»Ÿ

### é‡è¦è„šæœ¬
- `smart_processor_v5_1.py` - æ™ºèƒ½æ–‡æ¡£å¤„ç†å™¨ï¼ˆæœ€æ–°ç‰ˆï¼‰
- `batch_process_manager.py` - æ‰¹é‡å¤„ç†ç®¡ç†å™¨
- `milvus_dedup_script_v2.py` - å‘é‡æ•°æ®åº“å»é‡å·¥å…·
- `rag_query_interface.py` - RAGæŸ¥è¯¢äº¤äº’ç•Œé¢

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

- SQLæŸ¥è¯¢å“åº”ï¼š5-30ç§’
- RAGæŸ¥è¯¢å“åº”ï¼š3-15ç§’
- PDFå¤„ç†é€Ÿåº¦ï¼š30ç§’/æ–‡æ¡£
- å¹¶å‘æ”¯æŒï¼šå¤šç”¨æˆ·åŒæ—¶è®¿é—®
- ç³»ç»Ÿç¨³å®šæ€§ï¼šç”Ÿäº§ç¯å¢ƒéªŒè¯

## ğŸ” é«˜çº§åŠŸèƒ½

### æ–‡æ¡£å¤„ç†
- è‡ªåŠ¨è¯†åˆ«å’Œä¸‹è½½ä¸Šå¸‚å…¬å¸å…¬å‘ŠPDF
- æ™ºèƒ½æ–‡æœ¬æå–å’Œåˆ†å—ï¼ˆchunk_size=1000ï¼‰
- æ”¯æŒpdfplumberå’ŒPyPDF2åŒå¼•æ“
- OCRå¤±è´¥æ–‡ä»¶è‡ªåŠ¨è®°å½•

### æŸ¥è¯¢ä¼˜åŒ–
- æŸ¥è¯¢ç¼“å­˜æœºåˆ¶
- æ™ºèƒ½æŸ¥è¯¢è·¯ç”±
- ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
- å…ƒæ•°æ®å¢å¼ºæ£€ç´¢

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼è¯·ç¡®ä¿ï¼š
- éµå¾ªç°æœ‰ä»£ç é£æ ¼
- æ·»åŠ é€‚å½“çš„æµ‹è¯•
- æ›´æ–°ç›¸å…³æ–‡æ¡£

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ‘¥ ä½œè€…

[Your Name]

## ğŸ™ è‡´è°¢

- LangChainå›¢é˜Ÿæä¾›çš„ä¼˜ç§€æ¡†æ¶
- Tushareæä¾›çš„é‡‘èæ•°æ®æ¥å£
- DeepSeekæä¾›çš„LLM APIæ”¯æŒ
- BGE-M3æ¨¡å‹æä¾›çš„ä¸­æ–‡è¯­ä¹‰ç†è§£èƒ½åŠ›

## ğŸ“ è”ç³»æ–¹å¼

- GitHub Issues: [é“¾æ¥]
- Email: [é‚®ç®±]

---

â­ å¦‚æœè§‰å¾—è¿™ä¸ªé¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStaræ”¯æŒä¸€ä¸‹ï¼
"""
        readme_path = self.project_root / "README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        print(f"âœ… README.md æ–‡ä»¶å·²åˆ›å»º: {readme_path}")

    def execute_cleanup(self, dry_run: bool = True):
        """æ‰§è¡Œæ¸…ç†æ“ä½œ"""
        if dry_run:
            print("\nğŸ” æ¨¡æ‹Ÿæ‰§è¡Œï¼ˆä¸ä¼šçœŸæ­£åˆ é™¤/ç§»åŠ¨æ–‡ä»¶ï¼‰")
        else:
            print("\nâš ï¸  è­¦å‘Šï¼šå³å°†æ‰§è¡Œå®é™…çš„æ–‡ä»¶æ“ä½œï¼")
            confirm = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no): ")
            if confirm.lower() != 'yes':
                print("æ“ä½œå·²å–æ¶ˆ")
                return
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        dirs_to_create = [
            'scripts/analysis',
            'scripts/maintenance', 
            'scripts/tools',
            'scripts/tests',
            'scripts/debugging',
            'scripts/utils',
            'scripts/setup',
            'archive/fixes/sql_agent_fixes',
            'archive/fixes/api_response_fixes',
            'archive/fixes/timeout_fixes',
            'archive/fixes/import_fixes',
            'archive/fixes/enum_fixes',
            'archive/old_versions',
            'archive/other',
            'docs/project_status',
            'docs/examples'
        ]
        
        for dir_name in dirs_to_create:
            dir_path = self.project_root / dir_name
            if not dir_path.exists():
                if not dry_run:
                    dir_path.mkdir(parents=True, exist_ok=True)
                print(f"{'[æ¨¡æ‹Ÿ]' if dry_run else 'âœ…'} åˆ›å»ºç›®å½•: {dir_name}")
        
        # ç§»åŠ¨å¯é‡å¤ä½¿ç”¨çš„å·¥å…·
        tool_mappings = {
            'tushare-db-analyzer.py': 'scripts/analysis/db_analyzer.py',
            'analyze_announcement_titles.py': 'scripts/analysis/announcement_analyzer.py',
            'test_api_correct.py': 'scripts/tests/test_api.py',
            'test_agents.py': 'scripts/tests/',
            'verify_system.py': 'scripts/utils/system_check.py',
            'load_milvus_collection.py': 'scripts/tools/',
            'install_dependencies.py': 'scripts/setup/',
        }
        
        for src_name, dst_path in tool_mappings.items():
            src = self.project_root / src_name
            if src.exists():
                if dst_path.endswith('/'):
                    dst = self.project_root / dst_path / src_name
                else:
                    dst = self.project_root / dst_path
                if not dry_run:
                    shutil.move(str(src), str(dst))
                print(f"{'[æ¨¡æ‹Ÿ]' if dry_run else 'âœ…'} ç§»åŠ¨: {src_name} -> {dst_path}")
        
        # ç§»åŠ¨é¡¹ç›®çŠ¶æ€æ–‡æ¡£
        for doc in self.analysis_results['documentation']:
            if 'project_status' in doc:
                src = self.project_root / doc
                dst = self.project_root / 'docs' / 'project_status' / Path(doc).name
                if src.exists() and src != dst:
                    if not dry_run:
                        shutil.move(str(src), str(dst))
                    print(f"{'[æ¨¡æ‹Ÿ]' if dry_run else 'âœ…'} ç§»åŠ¨: {doc} -> docs/project_status/")
        
        # å½’æ¡£ä¸€æ¬¡æ€§è„šæœ¬
        fix_mappings = {
            'sql_agent': 'archive/fixes/sql_agent_fixes/',
            'api_response': 'archive/fixes/api_response_fixes/',
            'timeout': 'archive/fixes/timeout_fixes/',
            'import': 'archive/fixes/import_fixes/',
            'enum': 'archive/fixes/enum_fixes/',
            'processor': 'archive/old_versions/'
        }
        
        for archive_file in self.analysis_results['to_archive']:
            src = self.project_root / archive_file
            if src.exists():
                # ç¡®å®šç›®æ ‡ç›®å½•
                target_dir = 'archive/other/'
                for key, dir_path in fix_mappings.items():
                    if key in archive_file.lower():
                        target_dir = dir_path
                        break
                
                dst = self.project_root / target_dir / Path(archive_file).name
                if not dry_run:
                    shutil.move(str(src), str(dst))
                print(f"{'[æ¨¡æ‹Ÿ]' if dry_run else 'âœ…'} å½’æ¡£: {archive_file} -> {target_dir}")
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        for temp_file in self.analysis_results['to_delete']:
            file_path = self.project_root / temp_file
            if file_path.exists():
                if not dry_run:
                    if file_path.is_dir():
                        shutil.rmtree(file_path)
                    else:
                        file_path.unlink()
                print(f"{'[æ¨¡æ‹Ÿ]' if dry_run else 'âœ…'} åˆ é™¤: {temp_file}")
        
        if not dry_run:
            print("\nâœ… æ¸…ç†å®Œæˆï¼")
        else:
            print("\nğŸ’¡ æ¨¡æ‹Ÿå®Œæˆï¼ä½¿ç”¨ --execute å‚æ•°æ¥æ‰§è¡Œå®é™…æ“ä½œ")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Gitç‰ˆæœ¬ç®¡ç†å‡†å¤‡å·¥å…·")
    parser.add_argument("--analyze", action="store_true", help="åˆ†æé¡¹ç›®ç»“æ„")
    parser.add_argument("--create-gitignore", action="store_true", help="åˆ›å»º.gitignoreæ–‡ä»¶")
    parser.add_argument("--create-env-example", action="store_true", help="åˆ›å»º.env.exampleæ–‡ä»¶")
    parser.add_argument("--create-readme", action="store_true", help="åˆ›å»ºREADME.mdæ–‡ä»¶")
    parser.add_argument("--cleanup", action="store_true", help="æ‰§è¡Œæ¸…ç†æ“ä½œï¼ˆæ¨¡æ‹Ÿï¼‰")
    parser.add_argument("--execute", action="store_true", help="æ‰§è¡Œå®é™…çš„æ¸…ç†æ“ä½œ")
    parser.add_argument("--all", action="store_true", help="æ‰§è¡Œæ‰€æœ‰æ“ä½œ")
    
    args = parser.parse_args()
    
    preparer = ProjectGitPreparer()
    
    if args.all or args.analyze:
        preparer.analyze_project()
    
    if args.all or args.create_gitignore:
        preparer.create_gitignore()
    
    if args.all or args.create_env_example:
        preparer.create_env_example()
    
    if args.all or args.create_readme:
        preparer.create_readme()
    
    if args.cleanup or args.execute:
        preparer.execute_cleanup(dry_run=not args.execute)
    
    if not any(vars(args).values()):
        print("ä½¿ç”¨ --help æŸ¥çœ‹å¯ç”¨é€‰é¡¹")
        print("\nå¿«é€Ÿå¼€å§‹:")
        print("1. python project_git_prepare.py --analyze  # åˆ†æé¡¹ç›®")
        print("2. python project_git_prepare.py --all      # åˆ›å»ºæ‰€æœ‰å¿…è¦æ–‡ä»¶")
        print("3. python project_git_prepare.py --cleanup  # æ¨¡æ‹Ÿæ¸…ç†")
        print("4. python project_git_prepare.py --execute  # æ‰§è¡Œæ¸…ç†")

if __name__ == "__main__":
    main()
